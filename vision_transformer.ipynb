{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishaanth4969/vision-transformer/blob/main/vision_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCrZxHpfEQU1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "up42iUeXjCae",
        "outputId": "5a6ed66d-f702-4a78-97f5-6aec61232395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3sEI12dS5qx"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((144,144)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkRBXuMkS5l0",
        "outputId": "6d718eb0-40c6-401e-e670-4bfb7b34ea14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 directories and 0 images in '/content/drive/MyDrive/106121084_Nishaanth_SPML_SOP_TASK0.zip/archive (6)/seg_test'.\n",
            "There are 6 directories and 0 images in '/content/drive/MyDrive/106121084_Nishaanth_SPML_SOP_TASK0.zip/archive (6)/seg_test/seg_test'.\n",
            "There are 0 directories and 553 images in '/content/drive/MyDrive/106121084_Nishaanth_SPML_SOP_TASK0.zip/archive (6)/seg_test/seg_test/glacier'.\n",
            "There are 0 directories and 510 images in '/content/drive/MyDrive/106121084_Nishaanth_SPML_SOP_TASK0.zip/archive (6)/seg_test/seg_test/sea'.\n",
            "There are 0 directories and 501 images in '/content/drive/MyDrive/106121084_Nishaanth_SPML_SOP_TASK0.zip/archive (6)/seg_test/seg_test/street'.\n",
            "There are 0 directories and 474 images in '/content/drive/MyDrive/106121084_Nishaanth_SPML_SOP_TASK0.zip/archive (6)/seg_test/seg_test/forest'.\n",
            "There are 0 directories and 437 images in '/content/drive/MyDrive/106121084_Nishaanth_SPML_SOP_TASK0.zip/archive (6)/seg_test/seg_test/buildings'.\n",
            "There are 0 directories and 525 images in '/content/drive/MyDrive/106121084_Nishaanth_SPML_SOP_TASK0.zip/archive (6)/seg_test/seg_test/mountain'.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "def dir(dir_path):\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "dir('/content/drive/MyDrive/106121084_Nishaanth_SPML_SOP_TASK0.zip/archive (6)/seg_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3STz9g4DS5gJ",
        "outputId": "596ecaab-57cf-4118-ae9e-880ba9279104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data:\n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 14034\n",
            "    Root location: /content/drive/MyDrive/106121084_Nishaanth_SPML_SOP_TASK0.zip/archive (6)/seg_train/seg_train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(144, 144), interpolation=bilinear, max_size=None, antialias=warn)\n",
            "               ToTensor()\n",
            "           )\n",
            "Test data:\n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 3000\n",
            "    Root location: /content/drive/MyDrive/106121084_Nishaanth_SPML_SOP_TASK0.zip/archive (6)/seg_test/seg_test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(144, 144), interpolation=bilinear, max_size=None, antialias=warn)\n",
            "               ToTensor()\n",
            "           )\n"
          ]
        }
      ],
      "source": [
        "train_data = datasets.ImageFolder(root='/content/drive/MyDrive/106121084_Nishaanth_SPML_SOP_TASK0.zip/archive (6)/seg_train/seg_train',\n",
        "                                     transform=transform)\n",
        "test_data = datasets.ImageFolder(root='/content/drive/MyDrive/106121084_Nishaanth_SPML_SOP_TASK0.zip/archive (6)/seg_test/seg_test',\n",
        "                                    transform=transform)\n",
        "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV8pNBC1S5cU",
        "outputId": "907f5e19-ee28-417d-9114-dddb5c296365"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcdYg8zBS5Vz",
        "outputId": "1c58a357-1176-44af-dc64-ca4e1cca418d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7d8addb56e30>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7d8addb56500>)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=32,\n",
        "                              num_workers=os.cpu_count(),\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                             batch_size=32,\n",
        "                             num_workers=os.cpu_count(),\n",
        "                             shuffle=False)\n",
        "\n",
        "train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOoJu_nnS5Q_",
        "outputId": "aa4d5f7a-1bd2-462a-e0f2-d24c5dcd3d41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image shape: torch.Size([32, 3, 144, 144]) -> [batch_size, color_channels, height, width]\n",
            "Label shape: torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "img, label = next(iter(train_dataloader))\n",
        "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
        "print(f\"Label shape: {label.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ0INPjgnbCF",
        "outputId": "3e7fd6d1-c2e8-435a-da82-77c2eac510bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guf_H7ujna_m"
      },
      "outputs": [],
      "source": [
        "from einops.layers.torch import Rearrange\n",
        "from torch import Tensor\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels = 3, patch_size = 8, emb_size = 128):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
        "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.projection(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWZURWdOna8z"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, n_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.att = torch.nn.MultiheadAttention(embed_dim=dim,\n",
        "                                               num_heads=n_heads,\n",
        "                                               dropout=dropout)\n",
        "        self.q = torch.nn.Linear(dim, dim)\n",
        "        self.k = torch.nn.Linear(dim, dim)\n",
        "        self.v = torch.nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.q(x)\n",
        "        k = self.k(x)\n",
        "        v = self.v(x)\n",
        "        attn_output, attn_output_weights = self.att(x, x, x)\n",
        "        return attn_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwMHH7yOna6N"
      },
      "outputs": [],
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7nuDsE2na3l"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Sequential):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op-Pbv_HnalT"
      },
      "outputs": [],
      "source": [
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzIvw0tv_OH2",
        "outputId": "a3a46b3d-fe0e-4cfb-b201-33b45e095878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ViT(\n",
            "  (patch_embedding): PatchEmbedding(\n",
            "    (projection): Sequential(\n",
            "      (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=4, p2=4)\n",
            "      (1): Linear(in_features=48, out_features=32, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (layers): ModuleList(\n",
            "    (0-5): 6 x Sequential(\n",
            "      (0): ResidualAdd(\n",
            "        (fn): PreNorm(\n",
            "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (att): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
            "            )\n",
            "            (q): Linear(in_features=32, out_features=32, bias=True)\n",
            "            (k): Linear(in_features=32, out_features=32, bias=True)\n",
            "            (v): Linear(in_features=32, out_features=32, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): ResidualAdd(\n",
            "        (fn): PreNorm(\n",
            "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.1, inplace=False)\n",
            "            (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "            (4): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): Sequential(\n",
            "    (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    (1): Linear(in_features=32, out_features=37, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from einops import repeat\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, ch=3, img_size=144, patch_size=4, emb_dim=32,\n",
        "                n_layers=6, out_dim=37, dropout=0.1, heads=2):\n",
        "        super(ViT, self).__init__()\n",
        "\n",
        "        # Attributes\n",
        "        self.channels = ch\n",
        "        self.height = img_size\n",
        "        self.width = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Patching\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=ch,\n",
        "                                              patch_size=patch_size,\n",
        "                                              emb_size=emb_dim)\n",
        "        # Learnable params\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.randn(1, num_patches + 1, emb_dim))\n",
        "        self.cls_token = nn.Parameter(torch.rand(1, 1, emb_dim))\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(n_layers):\n",
        "            transformer_block = nn.Sequential(\n",
        "                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads = heads, dropout = dropout))),\n",
        "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout = dropout))))\n",
        "            self.layers.append(transformer_block)\n",
        "\n",
        "        # Classification head\n",
        "        self.head = nn.Sequential(nn.LayerNorm(emb_dim), nn.Linear(emb_dim, out_dim))\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Get patch embedding vectors\n",
        "        x = self.patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        # Add cls token to inputs\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "\n",
        "        # Transformer layers\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.layers[i](x)\n",
        "\n",
        "        # Output based on classification token\n",
        "        return self.head(x[:, 0, :])\n",
        "\n",
        "\n",
        "model = ViT()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvGcPZeQjVX8",
        "outputId": "415282c6-3495-4767-c22d-81be7a73efb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Epoch 0 train loss:  1.9068104218784931\n",
            ">>> Epoch 0 test loss:  1.7984064462337088\n",
            ">>> Epoch 5 train loss:  1.7971576205147155\n",
            ">>> Epoch 5 test loss:  1.801486778766551\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "model = ViT()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1000):\n",
        "    epoch_losses = []\n",
        "    model.train()\n",
        "    for step, (inputs, labels) in enumerate(train_dataloader):\n",
        "        inputs, labels = inputs, labels\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_losses.append(loss.item())\n",
        "    if epoch % 5 == 0:\n",
        "        print(f\">>> Epoch {epoch} train loss: \", np.mean(epoch_losses))\n",
        "        epoch_losses = []\n",
        "\n",
        "        for step, (inputs, labels) in enumerate(test_dataloader):\n",
        "            inputs, labels = inputs, labels\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            epoch_losses.append(loss.item())\n",
        "        print(f\"Epoch {epoch} test loss: \", np.mean(epoch_losses))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1nAspxCRdwuqCZ3apYFfFLPD-UA3kkeCs",
      "authorship_tag": "ABX9TyP0og5FVy6BGsdT9GIZqrF4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}